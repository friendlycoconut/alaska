{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install -q efficientnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import math, re, os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from kaggle_datasets import KaggleDatasets\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as L\n",
    "import efficientnet.tfkeras as efn\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "base_path = '/kaggle/input/alaska2-image-steganalysis/'\n",
    "algorithm = ('Cover(Unaltered)', 'JMiPOD', 'UERD', 'JUNIWARD')\n",
    "fig, axes = plt.subplots(nrows=4, ncols=4, figsize = (11,11) )\n",
    "np.random.seed(55)\n",
    "for i,id in enumerate(np.random.randint(0,75001,4)):\n",
    "    id = '{:05d}'.format(id)\n",
    "    cover_path = os.path.join(base_path, 'Cover', id + '.jpg')\n",
    "    jmipod_path = os.path.join(base_path, 'JMiPOD', id + '.jpg')\n",
    "    uerd_path = os.path.join(base_path, 'UERD', id + '.jpg')\n",
    "    juniward_path = os.path.join(base_path, 'JUNIWARD', id + '.jpg')\n",
    "    cover_img = plt.imread(cover_path)\n",
    "    jmipod_img = plt.imread(jmipod_path)\n",
    "    uerd_img = plt.imread(uerd_path)\n",
    "    juniward_img = plt.imread(juniward_path)\n",
    "    axes[i,0].imshow(cover_img)\n",
    "    axes[i,1].imshow(jmipod_img)\n",
    "    axes[i,2].imshow(uerd_img)\n",
    "    axes[i,3].imshow(juniward_img)\n",
    "    axes[i,0].set(ylabel=id+'.jpg')\n",
    "\n",
    "for i,algo in enumerate(algorithm):\n",
    "    axes[0,i].set(title=algo)\n",
    "for ax in axes.flat:\n",
    "    ax.set(xticks=[], yticks=[])\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cover_hist = {}\n",
    "jmipod_hist = {}\n",
    "uerd_hist = {}\n",
    "juniward_hist = {}\n",
    "color = ('b','g','r')\n",
    "for i,col in enumerate(color):\n",
    "    cover_hist[col] = cv2.calcHist([cover_img],[i],None,[256],[0,256])\n",
    "    jmipod_hist[col] = cv2.calcHist([jmipod_img],[i],None,[256],[0,256])\n",
    "    uerd_hist[col] = cv2.calcHist([uerd_img],[i],None,[256],[0,256])\n",
    "    juniward_hist[col] = cv2.calcHist([juniward_img],[i],None,[256],[0,256])\n",
    "\n",
    "fig_hist, axes_hist = plt.subplots(nrows=2, ncols=2, figsize=(12,12))\n",
    "for ax, hist, algo in zip(axes_hist.flat, [cover_hist, jmipod_hist, uerd_hist, juniward_hist], algorithm):\n",
    "    ax.plot(hist['r'], color = 'r', label='r')\n",
    "    ax.plot(hist['g'], color = 'g', label='g')\n",
    "    ax.plot(hist['b'], color = 'b', label='b')\n",
    "    ax.set(ylabel='# of pixels', xlabel='Pixel value(0-255)', title=algo)\n",
    "    ax.legend()\n",
    "fig_hist.subplots_adjust(wspace=0.4, hspace=0.3)\n",
    "fig_hist.suptitle('Histogram of a sample (' + id + '.jpg)', fontsize=20)\n",
    "    #     ax.xlim([0,256])\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "ax.plot(cover_hist['r'][50:80], color = 'c', label=algorithm[0])\n",
    "ax.plot(jmipod_hist['r'][50:80], color = 'm', label=algorithm[1])\n",
    "ax.plot(uerd_hist['r'][50:80], color = 'y', label=algorithm[2])\n",
    "ax.plot(juniward_hist['r'][50:80], color = 'g', label=algorithm[3])\n",
    "ax.legend()\n",
    "ax.set_ylabel('# of pixels', fontsize=15)\n",
    "ax.set_xlabel('Pixel value(50-80)', fontsize=15)\n",
    "ax.xaxis.set(ticklabels=np.linspace(50,80,8, dtype=np.int))\n",
    "ax.set_title('R-channel Histogram Compared (zoomed in)', fontsize=20)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=4, ncols=4, figsize = (11,11) )\n",
    "np.random.seed(55)\n",
    "def disp_diff_img(alt, ref, ax, chnl=0):\n",
    "    diff = np.abs(alt.astype(np.int)-ref.astype(np.int)).astype(np.uint8)\n",
    "    ax.imshow(diff[:,:,chnl], vmin=0, vmax=np.amax(diff[:,:,chnl]), cmap='hot')\n",
    "for i,id in enumerate(np.random.randint(0,75001,4)):\n",
    "    id = '{:05d}'.format(id)\n",
    "    cover_path = os.path.join(base_path, 'Cover', id + '.jpg')\n",
    "    jmipod_path = os.path.join(base_path, 'JMiPOD', id + '.jpg')\n",
    "    uerd_path = os.path.join(base_path, 'UERD', id + '.jpg')\n",
    "    juniward_path = os.path.join(base_path, 'JUNIWARD', id + '.jpg')\n",
    "    cover_img = plt.imread(cover_path)\n",
    "    jmipod_img = plt.imread(jmipod_path)\n",
    "    uerd_img = plt.imread(uerd_path)\n",
    "    juniward_img = plt.imread(juniward_path)\n",
    "    axes[i,0].imshow(cover_img)\n",
    "    disp_diff_img(jmipod_img, cover_img, axes[i,1], 0)\n",
    "    disp_diff_img(uerd_img, cover_img, axes[i,2], 0)\n",
    "    disp_diff_img(juniward_img, cover_img, axes[i,3], 0)\n",
    "    axes[i,0].set(ylabel=id+'.jpg')\n",
    "\n",
    "for i,algo in enumerate(algorithm):\n",
    "    axes[0,i].set(title=algo + 'diff')\n",
    "for ax in axes.flat:\n",
    "    ax.set(xticks=[], yticks=[])\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Detect hardware, return appropriate distribution strategy\n",
    "try:\n",
    "    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n",
    "    # set: this is always the case on Kaggle.\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    print('Running on TPU ', tpu.master())\n",
    "except ValueError:\n",
    "    tpu = None\n",
    "\n",
    "if tpu:\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "else:\n",
    "    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# For tf.dataset\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "# Data access\n",
    "GCS_DS_PATH = KaggleDatasets().get_gcs_path()\n",
    "\n",
    "# Configuration\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 16 * strategy.num_replicas_in_sync"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def append_path(pre):\n",
    "    return np.vectorize(lambda file: os.path.join(GCS_DS_PATH, pre, file))\n",
    "\n",
    "sub = pd.read_csv('alaska2-image-steganalysis/sample_submission.csv')\n",
    "train_filenames = np.array(os.listdir(\"alaska2-image-steganalysis/Cover/\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "positives = train_filenames.copy()\n",
    "negatives = train_filenames.copy()\n",
    "np.random.shuffle(positives)\n",
    "np.random.shuffle(negatives)\n",
    "\n",
    "jmipod = append_path('JMiPOD')(positives[:10000])\n",
    "juniward = append_path('JUNIWARD')(positives[10000:20000])\n",
    "uerd = append_path('UERD')(positives[20000:30000])\n",
    "\n",
    "pos_paths = np.concatenate([jmipod, juniward, uerd])np.random.seed(0)\n",
    "positives = train_filenames.copy()\n",
    "negatives = train_filenames.copy()\n",
    "np.random.shuffle(positives)\n",
    "np.random.shuffle(negatives)\n",
    "\n",
    "jmipod = append_path('JMiPOD')(positives[:10000])\n",
    "juniward = append_path('JUNIWARD')(positives[10000:20000])\n",
    "uerd = append_path('UERD')(positives[20000:30000])\n",
    "\n",
    "pos_paths = np.concatenate([jmipod, juniward, uerd])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_paths = append_path('Test')(sub.Id.values)\n",
    "neg_paths = append_path('Cover')(negatives[:30000])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_paths = np.concatenate([pos_paths, neg_paths])\n",
    "train_labels = np.array([1] * len(pos_paths) + [0] * len(neg_paths))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_paths, valid_paths, train_labels, valid_labels = train_test_split(\n",
    "    train_paths, train_labels, test_size=0.15, random_state=2020)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def decode_image(filename, label=None, image_size=(512, 512)):\n",
    "    bits = tf.io.read_file(filename)\n",
    "    image = tf.image.decode_jpeg(bits, channels=3)\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    image = tf.image.resize(image, image_size)\n",
    "\n",
    "    if label is None:\n",
    "        return image\n",
    "    else:\n",
    "        return image, label\n",
    "\n",
    "def data_augment(image, label=None):\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_flip_up_down(image)\n",
    "\n",
    "    if label is None:\n",
    "        return image\n",
    "    else:\n",
    "        return image, label"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_dataset = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((train_paths, train_labels))\n",
    "    .map(decode_image, num_parallel_calls=AUTO)\n",
    "    .cache()\n",
    "    .repeat()\n",
    "    .shuffle(1024)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(AUTO)\n",
    ")\n",
    "\n",
    "valid_dataset = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((valid_paths, valid_labels))\n",
    "    .map(decode_image, num_parallel_calls=AUTO)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .cache()\n",
    "    .prefetch(AUTO)\n",
    ")\n",
    "\n",
    "test_dataset = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices(test_paths)\n",
    "    .map(decode_image, num_parallel_calls=AUTO)\n",
    "    .batch(BATCH_SIZE)\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def build_lrfn(lr_start=0.00001, lr_max=0.000075,\n",
    "               lr_min=0.000001, lr_rampup_epochs=20,\n",
    "               lr_sustain_epochs=0, lr_exp_decay=.8):\n",
    "    lr_max = lr_max * strategy.num_replicas_in_sync\n",
    "\n",
    "    def lrfn(epoch):\n",
    "        if epoch < lr_rampup_epochs:\n",
    "            lr = (lr_max - lr_start) / lr_rampup_epochs * epoch + lr_start\n",
    "        elif epoch < lr_rampup_epochs + lr_sustain_epochs:\n",
    "            lr = lr_max\n",
    "        else:\n",
    "            lr = (lr_max - lr_min) * lr_exp_decay**(epoch - lr_rampup_epochs - lr_sustain_epochs) + lr_min\n",
    "        return lr\n",
    "\n",
    "    return lrfn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    model = tf.keras.Sequential([\n",
    "        efn.EfficientNetB3(\n",
    "            input_shape=(512, 512, 3),\n",
    "            weights='imagenet',\n",
    "            include_top=False\n",
    "        ),\n",
    "        L.GlobalAveragePooling2D(),\n",
    "        L.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss = 'binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "    # lrfn = build_lrfn()\n",
    "# lr_schedule = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=1)\n",
    "STEPS_PER_EPOCH = train_labels.shape[0] // BATCH_SIZE\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=EPOCHS,\n",
    "#     callbacks=[lr_schedule],\n",
    "    steps_per_epoch=STEPS_PER_EPOCH,\n",
    "    validation_data=valid_dataset\n",
    ")\n",
    "\n",
    "model.save(\"model.h5\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def display_training_curves(training, validation, title, subplot):\n",
    "    \"\"\"\n",
    "    Source: https://www.kaggle.com/mgornergoogle/getting-started-with-100-flowers-on-tpu\n",
    "    \"\"\"\n",
    "    if subplot%10==1: # set up the subplots on the first call\n",
    "        plt.subplots(figsize=(10,10), facecolor='#F0F0F0')\n",
    "        plt.tight_layout()\n",
    "    ax = plt.subplot(subplot)\n",
    "    ax.set_facecolor('#F8F8F8')\n",
    "    ax.plot(training)\n",
    "    ax.plot(validation)\n",
    "    ax.set_title('model '+ title)\n",
    "    ax.set_ylabel(title)\n",
    "    #ax.set_ylim(0.28,1.05)\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.legend(['train', 'valid.'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "display_training_curves(\n",
    "    history.history['loss'],\n",
    "    history.history['val_loss'],\n",
    "    'loss',211\n",
    ")\n",
    "\n",
    "display_training_curves(\n",
    "    history.history['accuracy'],\n",
    "    history.history['val_accuracy'],\n",
    "    'accuracy',212\n",
    ")\n",
    "\n",
    "\n",
    "sub.Label = model.predict(test_dataset, verbose=1)\n",
    "sub.to_csv('submission.csv', index=False)\n",
    "sub.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}